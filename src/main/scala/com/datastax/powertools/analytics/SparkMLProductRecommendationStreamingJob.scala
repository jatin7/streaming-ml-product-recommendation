package com.datastax.powertools.analytics

import org.apache.spark.{SparkConf, SparkContext, rdd}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}
import com.brkyvz.spark.recommendation.{LatentMatrixFactorization, StreamingLatentMatrixFactorization, StreamingLatentMatrixFactorizationModel}
import com.datastax.powertools.analytics.ddl.DSECapable
import org.apache.spark.ml.recommendation.ALS.Rating
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.streaming.dstream.DStream
import org.dmg.pmml.False


// For DSE it is not necessary to set connection parameters for spark.master (since it will be done
// automatically)

/**
 * https://github.com/brkyvz/streaming-matrix-factorization
 * https://issues.apache.org/jira/browse/SPARK-6407
 */
object SparkMLProductRecommendationStreamingJob extends DSECapable{

  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: SimpleSparkStreaming <hostname> <port>")
      System.exit(1)
    }

    // Create the context with a 1 second batch size
    val sc = connectToDSE("SparkMLProductRecommendationStreamingJob")
    val sqlContext = new SQLContext(sc)
    val ssc = new StreamingContext(sc, Seconds(1))

    //get the raw data
    val observations:RDD[Rating[Long]] = sqlContext.read.format("com.databricks.spark.csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .option("delimiter", "\t")
      .load("dsefs:///sales_observations").map(row =>
          Rating(row.getInt(0).toLong, row.getInt(1).toLong, row.getDouble(2).toFloat)
        )
      .cache()

    //train from the file
    //val algorithm = new StreamingLatentMatrixFactorization()
    val algorithm = new LatentMatrixFactorization()
    algorithm.trainOn(observations)

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val observation = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    observation.print

    def sampleRDD(fraction: Double, seed: Long)= {
      rdd: RDD[String] => rdd.sample(false, fraction,seed)
    }

    //Split the stream into training and testing datasets
    val train = observation.transform(sampleRDD(1, 10))
    val tests = observation.transform(sampleRDD(.2, 10))

    val trainStream= train.map(_.split("\t")).filter(x => (x.size >2 && x(0).forall(_.isDigit))).map(x =>
      Rating(x(0).toLong, x(1).toLong, x(2).toFloat)
    )
    //we could also train based on streaming but it blows up when there is an empty microbatch
    //algorithm.trainOn(trainStream)


    val testStream= tests.map(_.split("\t")).filter(x => (x.size >2 && x(0).forall(_.isDigit))).map(x =>
        (x(0).toLong, x(1).toLong)
    )
    //now predict against the live stream
    //this gives us predicted ratings for the item user combination fed from the stream
    val predictions: DStream[Rating[Long]] = algorithm.predictOn(testStream)

    predictions.map(x => "item: " + x.item + " user: " +  x.user+ " rating: " + x.rating).print()

    ssc.start()
    ssc.awaitTermination()
  }

  var conf: SparkConf = _
  var sc: SparkContext = _
}
// scalastyle:on println